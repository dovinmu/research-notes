# Performing surgery on a neural network

The scale of the largest neural networks, like GPT-3, has increased along with the cost of training them. It seems like scaling up has predictably beneficial results [[BD Tech Talks](https://bdtechtalks.com/2019/11/25/ai-research-neural-networks-compute-costs/)]. There are lots of issues that we have with how a neural network behaves, like GPT-3 generating dark, sexually explicit content without a clear provocation [[Wired](https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/)]. How much of the behavior of a neural network can we change after it has been trained? Would it be possible to remove that behavior of GPT-3 without retraining it on a new dataset (which would take a lot of compute)?

## Modifying a network after training

We can [scale NNs down](https://www.technologyreview.com/2019/05/10/135426/a-new-way-to-build-tiny-neural-networks-could-create-powerful-ai-on-your-phone/) 

## Using feature visualization to assign meaning

distill.pub has a number of great publications that visualize different aspects of neural nets.

[Multimodal neurons](https://distill.pub/2021/multimodal-neurons/) 

## Misc links

[Lambda Labs, demystifying GPT-3](https://lambdalabs.com/blog/demystifying-gpt-3/)

[2013 paper on word embeddings and vector offsets](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf) (word algebra)
